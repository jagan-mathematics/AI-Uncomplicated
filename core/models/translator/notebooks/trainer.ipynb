{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the root directory to the sys.path\n",
    "sys.path.insert(0, \"/root/AI-Uncomplicated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.translator.config import ModelConfig, DatasetConfig, TrainingConfig\n",
    "from core.dataloaders.dataloader import load_tokenizer\n",
    "\n",
    "## Initialize configurations\n",
    "model_config = ModelConfig(model_name=\"Construe\",\n",
    "                           num_layers = 2,\n",
    "                            padding_id = 0,\n",
    "                            hidden_dim = 512,\n",
    "                            intermediate_dim = 3072,\n",
    "                            max_positions = 2048,\n",
    "                            layer_norm_eps = 1e-05,\n",
    "                            model_max_sequence = 2048,\n",
    "                            num_heads = 8,\n",
    "                            attention_dropout = 0.1)\n",
    "\n",
    "dataset_config = DatasetConfig(dataset_path=\"./dataset\",\n",
    "                               dataset_shuffle=True)\n",
    "training_config = TrainingConfig(tokenizer_path=\"/root/AI-Uncomplicated/core/models/translator/tokenzier/european_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import sentencepiece as spm\n",
    "import core.utils.sentencepiece_model_pb2 as model_loader\n",
    "import torch\n",
    "import json\n",
    "\n",
    "class SPMTokenizer:\n",
    "    def __init__(self, tokenizer_path, bos_peice=\"<s>\", eos_peice=\"</s>\", padding_peice=\"<pad>\"):\n",
    "        self.path = tokenizer_path\n",
    "        self.model = spm.SentencePieceProcessor(model_file=os.path.join(tokenizer_path, \"spm_buffer.model\"))\n",
    "        \n",
    "        with open(os.path.join(tokenizer_path, \"tokenizer.config\"), \"r\") as handler:\n",
    "            self.config = json.load(handler)\n",
    "            \n",
    "        self.peices = [self.model.id_to_piece(id) for id in range(self.model.get_piece_size())]\n",
    "        self.unused_token_pattern = \"<unused\\d+>\"\n",
    "        \n",
    "        self.token_slots = self.config[\"available_unused_slots\"]\n",
    "        self.filled_slots = 0\n",
    "\n",
    "        self.special_tokens = self.config[\"special_tokens\"]\n",
    "        self.__special_token_slot_map = {}\n",
    "        \n",
    "        self.start_token_idx = self.model.PieceToId(bos_peice)\n",
    "        self.end_token_idx = self.model.PieceToId(eos_peice)\n",
    "        self.pad_token_idx = self.model.PieceToId(padding_peice)\n",
    "    \n",
    "    # def add_special_token(self, token_value):\n",
    "    #     if self.filled_slots < self.token_slots:\n",
    "    #         self.special_tokens.append(token_value)\n",
    "    #         self.__special_token_slot_map[f\"<unused{self.filled_slots}>\"] = token_value\n",
    "    #         self.filled_slots += 1\n",
    "    #     raise ValueError(\"Slot full\")\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def available_special_token_slots(self):\n",
    "        return self.token_slots\n",
    "    \n",
    "    \n",
    "    def get_unused_peices(self):\n",
    "        unused_peices = []\n",
    "        for peice in self.peices:\n",
    "            if re.match(self.unused_token_pattern, peice):\n",
    "                unused_peices.append(peice)\n",
    "        return unused_peices\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.model.vocab_size()\n",
    "\n",
    "    @vocab_size.setter\n",
    "    def vocab_size(self, value):\n",
    "        raise ValueError(\"Assigning value not supported\")\n",
    "    \n",
    "    def encode(self, inputs, add_special_tokens=True, return_type=\"pt\"):\n",
    "        if isinstance(inputs, str):\n",
    "            inputs = [inputs]\n",
    "        encoded_tokens = self.model.Encode(inputs)\n",
    "        if add_special_tokens:\n",
    "            for idx in range(len(encoded_tokens)):\n",
    "                encoded_tokens[idx] = [self.start_token_idx] + encoded_tokens[idx] + [self.end_token_idx]\n",
    "        \n",
    "        if return_type is None:\n",
    "            return {\"input_ids\": encoded_tokens, \"attention_mask\": None}\n",
    "        elif return_type == \"pt\":\n",
    "            paddded_tokens = torch.nn.utils.rnn.pad_sequence([torch.tensor(p) for p in encoded_tokens], batch_first=True, padding_value=self.pad_token_idx).long()\n",
    "            attention_mask = (paddded_tokens != self.pad_token_idx).to(torch.int32)\n",
    "            return {\"input_ids\": paddded_tokens, \"attention_maks\": attention_mask}\n",
    "        else:\n",
    "            raise ValueError(\"unsupported return type\")\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        if isinstance(tokens, torch.Tensor):\n",
    "            tokens = tokens.numpy().tolist()\n",
    "        return self.model.Decode(tokens)\n",
    "    \n",
    "    # def save_to_folder(self, folder_path, name=\"model\"):\n",
    "    #     m = model_loader.ModelProto()\n",
    "    #     m.ParseFromString(open(self.path , 'rb').read())\n",
    "    \n",
    "    #     fillable_peices = list(self.__special_token_slot_map.keys())\n",
    "    #     for p in m.pieces:\n",
    "    #         if p.piece in fillable_peices:\n",
    "    #             p.piece = self.__special_token_slot_map[p.piece]\n",
    "        \n",
    "        \n",
    "    #     with open(os.path.join(folder_path, f\"spm_buffer.model\"), 'wb') as f:\n",
    "    #         f.write(m.SerializeToString())\n",
    "        \n",
    "    #     with open(os.path.join(folder_path, f\"tokenizer.config\"), \"w\") as f:\n",
    "    #         config = {\n",
    "    #             \"vocab_size\": self.vocab_size,\n",
    "    #             \"available_unused_slots\": self.available_special_token_slots,\n",
    "    #             \"special_tokens\": self.special_tokens,\n",
    "    #         }\n",
    "    #         json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = SPMTokenizer(training_config.tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'what', 'is', 'zo', 'ho', 'cr', 'm']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.model.decode(i) for i in  [token.model.Encode(\"<s>\")[1]] + token.model.Encode(\"what is zoho crm\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     1,   3027,   1670,   2465,   1840,   2418, 108127,      2],\n",
       "        [     1,   2222,   1670,   2031,   1482,      2,      0,      0]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.encode([\"what is zoho crm\", \"this is jagan\"])[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.add_special_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.model.PieceToId(\"<pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in m.pieces:\n",
    "    if p.piece == \" ‚Åá\":\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_experimentation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
