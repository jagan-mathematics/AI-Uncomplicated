{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "ROOT_PROJECT_PATH = \"/root/AI-Uncomplicated\"\n",
    "# Add the root directory to the sys.path\n",
    "sys.path.insert(0, ROOT_PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = os.path.join(ROOT_PROJECT_PATH, \"core/models/translator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.translator.config import ModelConfig, DatasetConfig, TrainingConfig\n",
    "from core.dataloaders.dataloader import load_tokenizer\n",
    "\n",
    "## Initialize configurations\n",
    "model_config = ModelConfig(model_name=\"Construe\",\n",
    "                           num_layers = 2,\n",
    "                            padding_id = 0,\n",
    "                            hidden_dim = 128,\n",
    "                            intermediate_dim = 1024,\n",
    "                            max_positions = 2048,\n",
    "                            layer_norm_eps = 1e-05,\n",
    "                            model_max_sequence = 2048,\n",
    "                            num_heads = 8,\n",
    "                            attention_dropout = 0.1)\n",
    "\n",
    "dataset_config = DatasetConfig(dataset_path=\"./dataset\",\n",
    "                               dataset_shuffle=True)\n",
    "training_config = TrainingConfig(tokenizer_path=\"/root/AI-Uncomplicated/core/models/translator/tokenzier/european_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"dhruvildave/en-fr-translation-dataset\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_DOWNLOAD_PATH = os.path.join(WORKSPACE, \"dataset\", \"cc_100_en_fr\")\n",
    "\n",
    "# if not os.path.exists(DATASET_DOWNLOAD_PATH):\n",
    "#     os.makedirs(DATASET_DOWNLOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER_TRAIN_DATASET_NAME = \"statmt/cc100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DOWNLAD_SCRIPT_PATH =  os.path.join(ROOT_PROJECT_PATH, \"scripts/hf_data_downloader.py\")\n",
    "\n",
    "# !python $DATA_DOWNLAD_SCRIPT_PATH --dataset $TOKENIZER_TRAIN_DATASET_NAME --working_dir=$DATASET_DOWNLOAD_PATH --allowed_pattern=\"en/**/*.parquet,fr/**/*.parquet\" --revision=convert/parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_DOWNLOAD_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PROCESS_SCRIPT =  os.path.join(ROOT_PROJECT_PATH, \"data_processing/parquet/beam_text_writer.py\")\n",
    "# OUTPUT_PATH = os.path.join(DATASET_DOWNLOAD_PATH, \"processed_path\")\n",
    "# INPUT_PATH = os.path.join(DATASET_DOWNLOAD_PATH, \"statmt/cc100\")\n",
    "\n",
    "\n",
    "# !python $DATA_PROCESS_SCRIPT --input_path=$INPUT_PATH --chunk_size=100000 --output_path=$OUTPUT_PATH --languages=en,fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER_TRAINING_SCRIPT = os.path.join(ROOT_PROJECT_PATH, \"core/tokenizer/trainer.py\")\n",
    "# MODEL_NAME = \"en_fr_combined_tokenizer\"\n",
    "# TOKENIZER_SAVED_PATH = os.path.join(ROOT_PROJECT_PATH, \"core/models/translator/tokenzier/\")\n",
    "\n",
    "# !python $TOKENIZER_TRAINING_SCRIPT --data_dir=$OUTPUT_PATH --vocab_size=60000 --model_name=$MODEL_NAME --character_coverage=1.0 --num_threads=100 --output_dir=$TOKENIZER_SAVED_PATH --yaml_file_path=\"/root/AI-Uncomplicated/core/models/translator/tokenzier/config.yml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below command from project root directory to convert the smp format to custom format we are going to use for training\n",
    "\n",
    "```bash\n",
    "python -m core.tokenizer.setencepiece_to_tokenizer --model_path=core/models/translator/tokenzier/en_pt_combined_tokenizer.model --save_path=core/models/translator/tokenzier\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.tokenizer import SPMTokenizer\n",
    "from core.models.translator.construe import ConstrueAutoRegressiveModel\n",
    "\n",
    "training_config.tokenizer_path = \"/root/AI-Uncomplicated/core/models/translator/tokenzier/en_fr_combined_tokenizer\"\n",
    "\n",
    "tokenizer = SPMTokenizer(training_config.tokenizer_path)\n",
    "\n",
    "model_config.vocabulary_size = tokenizer.vocab_size\n",
    "model = ConstrueAutoRegressiveModel(config=model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntoPTDataSet(Dataset):\n",
    "    def __init__(self, tensorflow_dataset):\n",
    "        self.dataset = tensorflow_dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "try:\n",
    "    dataset = Dataset.from_csv(os.path.join(path, \"en-fr.csv\"))\n",
    "    dataset = dataset.take(3000000)\n",
    "    dataset = dataset.filter(lambda d: d[\"en\"] is not None and d[\"fr\"] is not None)\n",
    "    dataset = dataset.filter(lambda x: len(tokenizer.encode(x[\"en\"] + x[\"fr\"], return_type=None)[\"input_ids\"][0]) < 500)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    dataset.save_to_disk(\"filterd_dataset\")\n",
    "except NameError:\n",
    "    dataset = load_from_disk(\"filterd_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 2393178\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 598295\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples_pt = EntoPTDataSet(dataset[\"train\"])\n",
    "val_examples_pt = EntoPTDataSet(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from typing import Optional, Callable\n",
    "\n",
    "def create_data_loader(\n",
    "    dataset,\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True,\n",
    "    num_workers: int = 4,\n",
    "    pin_memory: bool = True,\n",
    "    collate_fn: Optional[Callable] = None,\n",
    "    drop_last: bool = False,\n",
    "    generator: Optional[torch.Generator] = None\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a PyTorch DataLoader with optimized settings.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PyTorch Dataset object\n",
    "        batch_size: Number of samples per batch\n",
    "        shuffle: Whether to shuffle the data\n",
    "        num_workers: Number of subprocesses for data loading\n",
    "        pin_memory: Whether to pin memory in GPU training\n",
    "        collate_fn: Custom collate function for batching\n",
    "        drop_last: Whether to drop the last incomplete batch\n",
    "        generator: Random number generator for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader: Configured PyTorch DataLoader\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if collate_fn is None:\n",
    "        raise ValueError(\"collator function not provided\")\n",
    "\n",
    "    # Choose sampler based on shuffle parameter\n",
    "    if shuffle:\n",
    "        sampler = RandomSampler(dataset, generator=generator)\n",
    "    else:\n",
    "        sampler = SequentialSampler(dataset)\n",
    "    \n",
    "    # Create DataLoader with optimized settings\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        # Worker init function for reproducibility\n",
    "        worker_init_fn=lambda worker_id: torch.manual_seed(torch.initial_seed() + worker_id)\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "\n",
    "class NextTokenPredictionCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.start = tokenizer.model.piece_to_id(\"<s>\")\n",
    "        self.end = tokenizer.model.piece_to_id(\"</s>\")\n",
    "        \n",
    "        self.en_start = tokenizer.model.piece_to_id(\"<lang_en>\")\n",
    "        self.en_end = tokenizer.model.piece_to_id(\"</lang_en>\")\n",
    "        self.pt_start = tokenizer.model.piece_to_id(\"<lang_pt>\")\n",
    "        self.pt_end = tokenizer.model.piece_to_id(\"</lang_pt>\")\n",
    "        self.pad_token_idx = tokenizer.model.PieceToId(\"<pad>\")\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        \n",
    "        for item in batch:\n",
    "            french, english = item[\"fr\"], item[\"en\"]\n",
    "            \n",
    "            english_encoded = tokenizer.encode(\n",
    "                english, return_type=None, add_special_tokens=False\n",
    "            )[\"input_ids\"][0]\n",
    "            \n",
    "            french_encoded = tokenizer.encode(\n",
    "                french, return_type=None, add_special_tokens=False\n",
    "            )[\"input_ids\"][0]\n",
    "            \n",
    "            \n",
    "            english_encoded = [self.start] + [self.en_start] +  english_encoded +  [self.en_end]\n",
    "            french_encoded = [self.pt_start] + french_encoded + [self.pt_end] + [self.end]\n",
    "            \n",
    "            input_ids.append(\n",
    "                english_encoded + french_encoded\n",
    "            )\n",
    "            labels.append(\n",
    "                (english_encoded + french_encoded)[1::] + [self.pad_token_idx]\n",
    "            )\n",
    "        \n",
    "        paddded_tokens = torch.nn.utils.rnn.pad_sequence([torch.tensor(p) for p in input_ids], batch_first=True, padding_value=self.pad_token_idx).long()\n",
    "        attention_mask = (paddded_tokens != self.pad_token_idx).to(torch.int32)\n",
    "        \n",
    "        target = torch.nn.utils.rnn.pad_sequence([torch.tensor(p) for p in labels], batch_first=True, padding_value=self.pad_token_idx).long()\n",
    "        target = torch.where(attention_mask == 0, -100, target)\n",
    "        \n",
    "        return {\"input_ids\": paddded_tokens, \"attention_mask\": attention_mask, \"labels\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = NextTokenPredictionCollator(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = create_data_loader(train_examples_pt, collate_fn=collate_fn, batch_size=dataset_config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> 6\n",
      "6 -> 4949\n",
      "4949 -> 17156\n",
      "17156 -> 6363\n",
      "6363 -> 43056\n",
      "43056 -> 30723\n",
      "30723 -> 5396\n",
      "5396 -> 6710\n",
      "6710 -> 6138\n",
      "6138 -> 9621\n",
      "9621 -> 21213\n",
      "21213 -> 7747\n",
      "7747 -> 52644\n",
      "52644 -> 52615\n",
      "52615 -> 3946\n",
      "3946 -> 28302\n",
      "28302 -> 20596\n",
      "20596 -> 52636\n",
      "52636 -> 7168\n",
      "7168 -> 9128\n",
      "9128 -> 1468\n",
      "1468 -> 52597\n",
      "52597 -> 2120\n",
      "2120 -> 2254\n",
      "2254 -> 1371\n",
      "1371 -> 41641\n",
      "41641 -> 17237\n",
      "17237 -> 6218\n",
      "6218 -> 36010\n",
      "36010 -> 1424\n",
      "1424 -> 19439\n",
      "19439 -> 52601\n",
      "52601 -> 7560\n",
      "7560 -> 1420\n",
      "1420 -> 7253\n",
      "7253 -> 5119\n",
      "5119 -> 52601\n",
      "52601 -> 6320\n",
      "6320 -> 1420\n",
      "1420 -> 41101\n",
      "41101 -> 7\n",
      "7 -> 8\n",
      "8 -> 7187\n",
      "7187 -> 30723\n",
      "30723 -> 52715\n",
      "52715 -> 4681\n",
      "4681 -> 17156\n",
      "17156 -> 6363\n",
      "6363 -> 14897\n",
      "14897 -> 1528\n",
      "1528 -> 52607\n",
      "52607 -> 19565\n",
      "19565 -> 52676\n",
      "52676 -> 21213\n",
      "21213 -> 7747\n",
      "7747 -> 7876\n",
      "7876 -> 4635\n",
      "4635 -> 1459\n",
      "1459 -> 3946\n",
      "3946 -> 20422\n",
      "20422 -> 3257\n",
      "3257 -> 30182\n",
      "30182 -> 52676\n",
      "52676 -> 9128\n",
      "9128 -> 1468\n",
      "1468 -> 52597\n",
      "52597 -> 2120\n",
      "2120 -> 2254\n",
      "2254 -> 1371\n",
      "1371 -> 17237\n",
      "17237 -> 52611\n",
      "52611 -> 19545\n",
      "19545 -> 16949\n",
      "16949 -> 1447\n",
      "1447 -> 1379\n",
      "1379 -> 52607\n",
      "52607 -> 7882\n",
      "7882 -> 1440\n",
      "1440 -> 11000\n",
      "11000 -> 1561\n",
      "1561 -> 7671\n",
      "7671 -> 1390\n",
      "1390 -> 43570\n",
      "43570 -> 6320\n",
      "6320 -> 1420\n",
      "1420 -> 41101\n",
      "41101 -> 9\n",
      "9 -> 2\n",
      "2 -> 0\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n"
     ]
    }
   ],
   "source": [
    "for l, i in zip(batch[\"labels\"][0].numpy().tolist(), batch[\"input_ids\"][0].numpy().tolist()):\n",
    "    print(f\"{i} -> {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(batch[\"attention_mask\"].numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config.warm_up = 4000\n",
    "training_config.logging_steps = 747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {key: value.to(\"cpu\") for key, value in batch.items()}\n",
    "pred = model(input_ids=batch[\"input_ids\"], attention_mask = batch[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def cal_loss(logits, gold, trg_pad_idx, smoothing=False):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "    n_classes = logits.shape[-1]\n",
    "    logits = logits.view(-1, n_classes)\n",
    "    gold = gold.unsqueeze(-1).view(-1)\n",
    "    loss = F.cross_entropy(logits, gold)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 402])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[\"logits\"].max(-1)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConstrueAutoRegressiveModel(\n",
       "  (model): ConstrueModel(\n",
       "    (token_embeddings): Embedding(60000, 128, padding_idx=0)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-1): 2 x ConstrueDecoderLayer(\n",
       "        (input_norm): LayerNorm()\n",
       "        (self_attn): RopeAttention(\n",
       "          (rope_position_projection): RopePositionEmbedding()\n",
       "          (qkv_projection): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (output_projection): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (post_attention_norm): LayerNorm()\n",
       "        (mlp): PointWiseGatedProjection(\n",
       "          (gate_projection): Linear(in_features=128, out_features=1024, bias=False)\n",
       "          (up_projection): Linear(in_features=128, out_features=1024, bias=False)\n",
       "          (down_projection): Linear(in_features=1024, out_features=128, bias=False)\n",
       "          (act_func): PytorchGELUTanh()\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=128, out_features=60000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_performance(pred, gold, trg_pad_idx=-100):\n",
    "    ''' Apply label smoothing if needed '''\n",
    "\n",
    "    loss = cal_loss(pred, gold, trg_pad_idx)\n",
    "\n",
    "    pred = pred.max(-1)[1].view(-1)\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    non_pad_mask = gold.ne(trg_pad_idx)\n",
    "    n_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\n",
    "    n_word = non_pad_mask.sum().item()\n",
    "\n",
    "    return loss, n_correct, n_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "from typing import Optional, List\n",
    "\n",
    "class WarmupCosineScheduler(LambdaLR):\n",
    "    \"\"\"Linear warmup and cosine decay scheduler.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: AdamW,\n",
    "        warmup_steps: int,\n",
    "        total_steps: int,\n",
    "        min_lr_ratio: float = 0.1,\n",
    "        last_epoch: int = -1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize warmup and decay scheduler.\n",
    "        \n",
    "        Args:\n",
    "            optimizer: AdamW optimizer\n",
    "            warmup_steps: Number of warmup steps\n",
    "            total_steps: Total number of training steps\n",
    "            min_lr_ratio: Minimum learning rate ratio compared to initial lr\n",
    "            last_epoch: The index of the last epoch\n",
    "        \"\"\"\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.min_lr_ratio = min_lr_ratio\n",
    "        super().__init__(optimizer, self.lr_lambda, last_epoch)\n",
    "    \n",
    "    def lr_lambda(self, current_step: int) -> float:\n",
    "        \"\"\"Calculate lr multiplier based on current step.\"\"\"\n",
    "        if current_step < self.warmup_steps:\n",
    "            # Linear warmup\n",
    "            return float(current_step) / float(max(1, self.warmup_steps))\n",
    "        \n",
    "        # Cosine decay\n",
    "        progress = float(current_step - self.warmup_steps) / \\\n",
    "            float(max(1, self.total_steps - self.warmup_steps))\n",
    "        decay = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "        # Scale decay to min_lr_ratio\n",
    "        decay = self.min_lr_ratio + (1.0 - self.min_lr_ratio) * decay\n",
    "        return decay\n",
    "\n",
    "def create_optimizer_and_scheduler(\n",
    "    model: torch.nn.Module,\n",
    "    num_training_steps: int,\n",
    "    learning_rate: float = 5e-5,\n",
    "    weight_decay: float = 0.01,\n",
    "    warmup_ratio: float = 0.1,\n",
    "    min_lr_ratio: float = 0.1,\n",
    "    beta1: float = 0.9,\n",
    "    beta2: float = 0.999,\n",
    "    eps: float = 1e-8,\n",
    "    no_decay_params: Optional[List[str]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create AdamW optimizer and warmup scheduler.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        num_training_steps: Total number of training steps\n",
    "        learning_rate: Maximum learning rate after warmup\n",
    "        weight_decay: Weight decay coefficient\n",
    "        warmup_ratio: Ratio of warmup steps to total steps\n",
    "        min_lr_ratio: Minimum learning rate ratio compared to max lr\n",
    "        beta1: AdamW beta1 parameter\n",
    "        beta2: AdamW beta2 parameter\n",
    "        eps: AdamW epsilon parameter\n",
    "        correct_bias: Whether to correct bias in AdamW\n",
    "        no_decay_params: List of parameter names that should not have weight decay\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (optimizer, scheduler)\n",
    "    \"\"\"\n",
    "    # Default params that should not have weight decay\n",
    "    if no_decay_params is None:\n",
    "        no_decay_params = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "    \n",
    "    # Separate parameters that should and should not have weight decay\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay_params)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay_params)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Create AdamW optimizer\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        betas=(beta1, beta2),\n",
    "        eps=eps\n",
    "    )\n",
    "    \n",
    "    # Create scheduler with linear warmup and cosine decay\n",
    "    print(f\"NUM Training Step :: {num_training_steps}\")\n",
    "    \n",
    "    warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "    print(f\"warmup_steps :: {warmup_steps}\")\n",
    "    \n",
    "    scheduler = WarmupCosineScheduler(\n",
    "        optimizer=optimizer,\n",
    "        warmup_steps=warmup_steps,\n",
    "        total_steps=num_training_steps,\n",
    "        min_lr_ratio=min_lr_ratio\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler, warmup_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_step(model, batch, optimizer, scheduler, device, logger=None, gradient_accumulation_steps=1):\n",
    "    # forward\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(input_ids=batch[\"input_ids\"].to(device), attention_mask=batch[\"attention_mask\"].to(device))\n",
    "    logits = pred[\"logits\"]\n",
    "    # backward and update parameters\n",
    "    loss, n_correct, n_word = cal_performance(logits, labels) \n",
    "    loss = loss / gradient_accumulation_steps\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    # if (optimizer.step_count + 1) % gradient_accumulation_steps == 0:\n",
    "    #     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    #     optimizer.step()\n",
    "    #     scheduler.step()\n",
    "    #     optimizer.zero_grad()\n",
    "    \n",
    "    return loss.item() * gradient_accumulation_steps, n_correct, n_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_epoch(epoch, model, training_data, optimizer, scheduler, device, logging_steps=-1, logger=None, gradient_accumulation_steps=1):\n",
    "    ''' Epoch operation in training phase'''\n",
    "    model.train()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n",
    "\n",
    "    desc = '  - (Training)   '\n",
    "    for step, batch in tqdm(enumerate(training_data), mininterval=2, desc=desc, leave=False, total=len(train_dataloader)):\n",
    "        # forward\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(input_ids=batch[\"input_ids\"].to(device), attention_mask=batch[\"attention_mask\"].to(device))\n",
    "        logits = pred[\"logits\"]\n",
    "        # backward and update parameters\n",
    "        loss, n_correct, n_word = cal_performance(logits, labels) \n",
    "        \n",
    "        loss.backward()\n",
    "        if (optimizer.step_count + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # note keeping\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        logger.log_metrics(metrics={\n",
    "            'loss': loss.item(),\n",
    "            'epoch': epoch,\n",
    "            \"n_correct\": n_correct,\n",
    "            \"n_word\": n_word,\n",
    "            'learning_rate': sched.get_current_learning_rate()\n",
    "        }, step=step)\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_training_steps(\n",
    "    num_examples: int,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    gradient_accumulation_steps: int = 1\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Calculate the total number of training steps.\n",
    "    \n",
    "    Args:\n",
    "        num_examples: Total number of training examples\n",
    "        num_epochs: Number of epochs to train for\n",
    "        batch_size: Batch size per forward pass\n",
    "        gradient_accumulation_steps: Number of steps to accumulate gradients\n",
    "    \n",
    "    Returns:\n",
    "        int: Total number of optimizer update steps\n",
    "    \"\"\"\n",
    "    \n",
    "    update_steps_per_epoch = math.ceil(num_examples / gradient_accumulation_steps)\n",
    "    \n",
    "    total_training_steps = update_steps_per_epoch * num_epochs\n",
    "    \n",
    "    return total_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class TrainingLogger:\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dir: str,\n",
    "        project_name: str,\n",
    "        log_every_n_steps: int = 100,\n",
    "        save_every_n_steps: int = 1000,\n",
    "        save_best_only: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize training logger with various logging options.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save checkpoints and logs\n",
    "            project_name: Name of the project\n",
    "            use_wandb: Whether to use Weights & Biases logging\n",
    "            log_every_n_steps: How often to log metrics\n",
    "            save_every_n_steps: How often to save checkpoints\n",
    "            save_best_only: Whether to save only the best model\n",
    "        \"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize logging\n",
    "        self.log_file = self.output_dir / 'training.log'\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # Training state\n",
    "        self.global_step = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.start_time = time.time()\n",
    "        self.last_log_time = self.start_time\n",
    "        \n",
    "        # Configuration\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "        self.save_every_n_steps = save_every_n_steps\n",
    "            \n",
    "        # Save configuration\n",
    "        self.save_config({\n",
    "            'output_dir': str(output_dir),\n",
    "            'project_name': project_name,\n",
    "            'log_every_n_steps': log_every_n_steps,\n",
    "            'save_every_n_steps': save_every_n_steps,\n",
    "            'save_best_only': save_best_only,\n",
    "            'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        })\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup logging configuration.\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(self.log_file),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def save_config(self, config: Dict[str, Any]):\n",
    "        \"\"\"Save configuration to JSON file.\"\"\"\n",
    "        config_path = self.output_dir / 'config.json'\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "    \n",
    "    def log_metrics(\n",
    "        self,\n",
    "        metrics: Dict[str, float],\n",
    "        step: Optional[int] = None,\n",
    "        force_log: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Log metrics to all configured outputs.\n",
    "        \n",
    "        Args:\n",
    "            metrics: Dictionary of metric names and values\n",
    "            step: Optional step number (uses global_step if not provided)\n",
    "            force_log: Whether to log regardless of log_every_n_steps\n",
    "        \"\"\"\n",
    "        if step is not None:\n",
    "            self.global_step = step\n",
    "        \n",
    "        # Check if we should log\n",
    "        if not force_log and self.global_step % self.log_every_n_steps != 0:\n",
    "            return\n",
    "        \n",
    "        # Calculate time statistics\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - self.start_time\n",
    "        elapsed_since_last = current_time - self.last_log_time\n",
    "        steps_since_last = self.log_every_n_steps\n",
    "        steps_per_second = steps_since_last / elapsed_since_last\n",
    "        \n",
    "        # Add timing metrics\n",
    "        metrics.update({\n",
    "            'elapsed_time': elapsed,\n",
    "            'steps_per_second': steps_per_second\n",
    "        })\n",
    "        \n",
    "        # Log to terminal and file\n",
    "        log_str = f'Step {self.global_step}: ' + ', '.join(\n",
    "            f'{k}: {v}' for k, v in metrics.items()\n",
    "        )\n",
    "        logging.info(log_str)\n",
    "        \n",
    "        self.last_log_time = current_time\n",
    "    \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        loss: float,\n",
    "        extra_data: Optional[Dict[str, Any]] = None,\n",
    "        force_save: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save model checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model\n",
    "            optimizer: PyTorch optimizer\n",
    "            loss: Current loss value\n",
    "            extra_data: Additional data to save in checkpoint\n",
    "            force_save: Whether to save regardless of save_every_n_steps\n",
    "        \"\"\"\n",
    "        # Check if we should save\n",
    "        should_save = (\n",
    "            force_save or\n",
    "            self.global_step % self.save_every_n_steps == 0 or\n",
    "            (self.save_best_only and loss < self.best_loss)\n",
    "        )\n",
    "        \n",
    "        if not should_save:\n",
    "            return\n",
    "        \n",
    "        # Update best loss if needed\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "        \n",
    "        # Prepare checkpoint data\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': self.global_step,\n",
    "            'loss': loss,\n",
    "            'best_loss': self.best_loss\n",
    "        }\n",
    "        \n",
    "        if extra_data:\n",
    "            checkpoint.update(extra_data)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = self.output_dir / f'checkpoint_{self.global_step}.pt'\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        logging.info(f'Saved checkpoint at step {self.global_step}')\n",
    "    \n",
    "    def finish(self):\n",
    "        \"\"\"Cleanup and final logging.\"\"\"\n",
    "        total_time = time.time() - self.start_time\n",
    "        logging.info(f'Training finished. Total time: {total_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(experimentation_name,\n",
    "          output_dir,\n",
    "          logging_steps,\n",
    "          save_steps,\n",
    "          num_epochs,\n",
    "          train_dataset,\n",
    "          collate_fn,\n",
    "          batch_size,\n",
    "          gradient_accumulation_steps=1, # not yet implemented\n",
    "          learning_rate=5e-5,\n",
    "          warmup_ratio=0.2,\n",
    "          weight_decay=0.0,\n",
    "          device=\"cuda\"):\n",
    "    \n",
    "    # prepare model\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    train_dataloader = create_data_loader(train_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
    "    \n",
    "    num_training_steps = calculate_training_steps(len(train_dataloader), num_epochs=num_epochs, batch_size=batch_size, gradient_accumulation_steps=gradient_accumulation_steps)\n",
    "\n",
    "    # Create optimizer and scheduler\n",
    "    optimizer, scheduler, warmup_steps = create_optimizer_and_scheduler(\n",
    "        model=model,\n",
    "        num_training_steps=num_training_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    train_logger = TrainingLogger(\n",
    "        project_name=experimentation_name,\n",
    "        output_dir=output_dir,\n",
    "        log_every_n_steps=logging_steps,\n",
    "        save_every_n_steps=save_steps,\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        desc = f'  - (Training)   Epoch {epoch}'\n",
    "        n_word_total, n_word_correct, total_loss = 0, 0, 0\n",
    "        for step, batch in tqdm(enumerate(train_dataloader), mininterval=2, desc=desc, leave=False, total=len(train_dataloader)):\n",
    "            loss, n_correct, n_word = train_step(\n",
    "                model=model,\n",
    "                batch=batch,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "            total_loss += loss\n",
    "            \n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            train_logger.log_metrics(metrics={\n",
    "                'loss': loss,\n",
    "                'epoch': epoch,\n",
    "                \"n_correct\": n_correct,\n",
    "                \"n_word\": n_word,\n",
    "                'learning_rate': current_lr,\n",
    "                \"warmup_steps\": warmup_steps\n",
    "            }, step=step)\n",
    "        print()\n",
    "        print(f\"Epoch {epoch} :: loss per word :: {total_loss/n_word_total}\")\n",
    "    train_logger.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM Training Step :: 149574\n",
      "warmup_steps :: 14957\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3c410d160a401b9d1626f45fe11adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  - (Training)   Epoch 0:   0%|          | 0/74787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 13:05:47,320 - INFO - Step 0: loss: 11.160490989685059, epoch: 0, n_correct: 0, n_word: 2685, learning_rate: 2.0057498161396e-08, warmup_steps: 14957, elapsed_time: 0.6764383316040039, steps_per_second: 1478.3313618977663\n"
     ]
    }
   ],
   "source": [
    "dataset_config.batch_size\n",
    "\n",
    "\n",
    "collate_fn = NextTokenPredictionCollator(tokenizer=tokenizer)\n",
    "\n",
    "training_config.logging_steps = 1000\n",
    "training_config.warm_up = 4000\n",
    "\n",
    "train(\n",
    "    experimentation_name=training_config.experimentation_name,\n",
    "    output_dir=training_config.save_path,\n",
    "    logging_steps=training_config.logging_steps,\n",
    "    save_steps=training_config.logging_steps,\n",
    "    num_epochs= 2, # training_config.num_epochs,\n",
    "    train_dataset=train_examples_pt,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=dataset_config.batch_size,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_ratio=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"weights.pt\", \"wb\") as handler:\n",
    "    torch.save(model.state_dict(), handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_390491/2733062557.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(handler)\n"
     ]
    }
   ],
   "source": [
    "with open(\"weights.pt\", \"rb\") as handler:\n",
    "    state_dict = torch.load(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"how to enable zia's email intelligence features in zoho crm\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=False, return_type=None)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.model.PieceToId(\"<s>\")] + [tokenizer.model.PieceToId(\"<lang_en>\")] +  input_ids[0] + [tokenizer.model.PieceToId(\"</lang_en>\")] + [tokenizer.model.PieceToId(\"<lang_pt>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.as_tensor([input_ids])\n",
    "# input_ids = input_ids.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     6,  1895,  1409,  9256,  3073,  1966, 52605, 52583,  3980,\n",
       "         10368,  6032,  1413,  3073,  3248, 52586,  1842, 52593,     7,     8]])"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"how to enable zia's email intelligence features in zoho crm Comment faire preuve de l'information sur le savoir-faire de l'information sur les produits de l'information\"]"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = outputs[\"logits\"][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-17.3053, -13.8722, -12.7217,  ..., -13.8961, -13.6052, -13.5213]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9, device='cuda:0')"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(next_token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</lang_pt>'"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.id_to_piece(torch.argmax(next_token_logits).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.hstack([input_ids, torch.as_tensor([[torch.argmax(next_token_logits).item()]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_experimentation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
