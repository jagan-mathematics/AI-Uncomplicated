{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "ROOT_PROJECT_PATH = \"/root/AI-Uncomplicated\"\n",
    "# Add the root directory to the sys.path\n",
    "sys.path.insert(0, ROOT_PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = os.path.join(ROOT_PROJECT_PATH, \"core/models/translator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.translator.config import ModelConfig, DatasetConfig, TrainingConfig\n",
    "from core.dataloaders.dataloader import load_tokenizer\n",
    "\n",
    "## Initialize configurations\n",
    "model_config = ModelConfig(model_name=\"Construe\",\n",
    "                           num_layers = 2,\n",
    "                            padding_id = 0,\n",
    "                            hidden_dim = 128,\n",
    "                            intermediate_dim = 1024,\n",
    "                            max_positions = 2048,\n",
    "                            layer_norm_eps = 1e-05,\n",
    "                            model_max_sequence = 2048,\n",
    "                            num_heads = 8,\n",
    "                            attention_dropout = 0.1)\n",
    "\n",
    "dataset_config = DatasetConfig(dataset_path=\"./dataset\",\n",
    "                               dataset_shuffle=True)\n",
    "training_config = TrainingConfig(tokenizer_path=\"/root/AI-Uncomplicated/core/models/translator/tokenzier/european_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /root/.cache/kagglehub/datasets/dhruvildave/en-fr-translation-dataset/versions/2\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"dhruvildave/en-fr-translation-dataset\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_DOWNLOAD_PATH = os.path.join(WORKSPACE, \"dataset\", \"cc_100_en_fr\")\n",
    "\n",
    "# if not os.path.exists(DATASET_DOWNLOAD_PATH):\n",
    "#     os.makedirs(DATASET_DOWNLOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER_TRAIN_DATASET_NAME = \"statmt/cc100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DOWNLAD_SCRIPT_PATH =  os.path.join(ROOT_PROJECT_PATH, \"scripts/hf_data_downloader.py\")\n",
    "\n",
    "# !python $DATA_DOWNLAD_SCRIPT_PATH --dataset $TOKENIZER_TRAIN_DATASET_NAME --working_dir=$DATASET_DOWNLOAD_PATH --allowed_pattern=\"en/**/*.parquet,fr/**/*.parquet\" --revision=convert/parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_DOWNLOAD_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PROCESS_SCRIPT =  os.path.join(ROOT_PROJECT_PATH, \"data_processing/parquet/beam_text_writer.py\")\n",
    "# OUTPUT_PATH = os.path.join(DATASET_DOWNLOAD_PATH, \"processed_path\")\n",
    "# INPUT_PATH = os.path.join(DATASET_DOWNLOAD_PATH, \"statmt/cc100\")\n",
    "\n",
    "\n",
    "# !python $DATA_PROCESS_SCRIPT --input_path=$INPUT_PATH --chunk_size=100000 --output_path=$OUTPUT_PATH --languages=en,fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER_TRAINING_SCRIPT = os.path.join(ROOT_PROJECT_PATH, \"core/tokenizer/trainer.py\")\n",
    "# MODEL_NAME = \"en_fr_combined_tokenizer\"\n",
    "# TOKENIZER_SAVED_PATH = os.path.join(ROOT_PROJECT_PATH, \"core/models/translator/tokenzier/\")\n",
    "\n",
    "# !python $TOKENIZER_TRAINING_SCRIPT --data_dir=$OUTPUT_PATH --vocab_size=60000 --model_name=$MODEL_NAME --character_coverage=1.0 --num_threads=100 --output_dir=$TOKENIZER_SAVED_PATH --yaml_file_path=\"/root/AI-Uncomplicated/core/models/translator/tokenzier/config.yml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below command from project root directory to convert the smp format to custom format we are going to use for training\n",
    "\n",
    "```bash\n",
    "python -m core.tokenizer.setencepiece_to_tokenizer --model_path=core/models/translator/tokenzier/en_pt_combined_tokenizer.model --save_path=core/models/translator/tokenzier\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.tokenizer import SPMTokenizer\n",
    "from core.models.translator.construe import ConstrueAutoRegressiveModel\n",
    "\n",
    "training_config.tokenizer_path = \"/root/AI-Uncomplicated/core/models/translator/tokenzier/en_fr_combined_tokenizer\"\n",
    "\n",
    "tokenizer = SPMTokenizer(training_config.tokenizer_path)\n",
    "\n",
    "model_config.vocabulary_size = tokenizer.vocab_size\n",
    "model = ConstrueAutoRegressiveModel(config=model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntoPTDataSet(Dataset):\n",
    "    def __init__(self, tensorflow_dataset):\n",
    "        self.dataset = tensorflow_dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "try:\n",
    "    dataset = Dataset.from_csv(os.path.join(path, \"en-fr.csv\"))\n",
    "    dataset = dataset.take(3000000)\n",
    "    dataset = dataset.filter(lambda d: d[\"en\"] is not None and d[\"fr\"] is not None)\n",
    "    dataset = dataset.filter(lambda x: len(tokenizer.encode(x[\"en\"] + x[\"fr\"], return_type=None)[\"input_ids\"][0]) < 500)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    dataset.save_to_disk(\"filterd_dataset\")\n",
    "except NameError:\n",
    "    dataset = load_from_disk(\"filterd_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 2393178\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 598295\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples_pt = EntoPTDataSet(dataset[\"train\"])\n",
    "val_examples_pt = EntoPTDataSet(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from typing import Optional, Callable\n",
    "\n",
    "def create_data_loader(\n",
    "    dataset,\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True,\n",
    "    num_workers: int = 4,\n",
    "    pin_memory: bool = True,\n",
    "    collate_fn: Optional[Callable] = None,\n",
    "    drop_last: bool = False,\n",
    "    generator: Optional[torch.Generator] = None\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a PyTorch DataLoader with optimized settings.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PyTorch Dataset object\n",
    "        batch_size: Number of samples per batch\n",
    "        shuffle: Whether to shuffle the data\n",
    "        num_workers: Number of subprocesses for data loading\n",
    "        pin_memory: Whether to pin memory in GPU training\n",
    "        collate_fn: Custom collate function for batching\n",
    "        drop_last: Whether to drop the last incomplete batch\n",
    "        generator: Random number generator for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader: Configured PyTorch DataLoader\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if collate_fn is None:\n",
    "        raise ValueError(\"collator function not provided\")\n",
    "\n",
    "    # Choose sampler based on shuffle parameter\n",
    "    if shuffle:\n",
    "        sampler = RandomSampler(dataset, generator=generator)\n",
    "    else:\n",
    "        sampler = SequentialSampler(dataset)\n",
    "    \n",
    "    # Create DataLoader with optimized settings\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        # Worker init function for reproducibility\n",
    "        worker_init_fn=lambda worker_id: torch.manual_seed(torch.initial_seed() + worker_id)\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "\n",
    "class NextTokenPredictionCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.start = tokenizer.model.piece_to_id(\"<s>\")\n",
    "        self.end = tokenizer.model.piece_to_id(\"</s>\")\n",
    "        \n",
    "        self.en_start = tokenizer.model.piece_to_id(\"<lang_en>\")\n",
    "        self.en_end = tokenizer.model.piece_to_id(\"</lang_en>\")\n",
    "        self.pt_start = tokenizer.model.piece_to_id(\"<lang_pt>\")\n",
    "        self.pt_end = tokenizer.model.piece_to_id(\"</lang_pt>\")\n",
    "        self.pad_token_idx = tokenizer.model.PieceToId(\"<pad>\")\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        \n",
    "        for item in batch:\n",
    "            french, english = item[\"fr\"], item[\"en\"]\n",
    "            \n",
    "            english_encoded = tokenizer.encode(\n",
    "                english, return_type=None, add_special_tokens=False\n",
    "            )[\"input_ids\"][0]\n",
    "            \n",
    "            french_encoded = tokenizer.encode(\n",
    "                french, return_type=None, add_special_tokens=False\n",
    "            )[\"input_ids\"][0]\n",
    "            \n",
    "            \n",
    "            english_encoded = [self.start] + [self.en_start] +  english_encoded +  [self.en_end]\n",
    "            french_encoded = [self.pt_start] + french_encoded + [self.pt_end] + [self.end]\n",
    "            \n",
    "            input_ids.append(\n",
    "                english_encoded + french_encoded\n",
    "            )\n",
    "            labels.append(\n",
    "                (english_encoded + french_encoded)[1::] + [self.pad_token_idx]\n",
    "            )\n",
    "        \n",
    "        paddded_tokens = torch.nn.utils.rnn.pad_sequence([torch.tensor(p) for p in input_ids], batch_first=True, padding_value=self.pad_token_idx).long()\n",
    "        attention_mask = (paddded_tokens != self.pad_token_idx).to(torch.int32)\n",
    "        \n",
    "        target = torch.nn.utils.rnn.pad_sequence([torch.tensor(p) for p in labels], batch_first=True, padding_value=self.pad_token_idx).long()\n",
    "        target = torch.where(attention_mask == 0, -100, target)\n",
    "        \n",
    "        return {\"input_ids\": paddded_tokens, \"attention_mask\": attention_mask, \"labels\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = NextTokenPredictionCollator(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = create_data_loader(train_examples_pt, collate_fn=collate_fn, batch_size=dataset_config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> 6\n",
      "6 -> 21743\n",
      "21743 -> 3458\n",
      "3458 -> 9345\n",
      "9345 -> 1420\n",
      "1420 -> 2132\n",
      "2132 -> 2701\n",
      "2701 -> 8304\n",
      "8304 -> 6921\n",
      "6921 -> 27895\n",
      "27895 -> 1424\n",
      "1424 -> 13901\n",
      "13901 -> 52597\n",
      "52597 -> 7\n",
      "7 -> 8\n",
      "8 -> 2564\n",
      "2564 -> 9881\n",
      "9881 -> 38594\n",
      "38594 -> 1512\n",
      "1512 -> 6923\n",
      "6923 -> 8992\n",
      "8992 -> 24439\n",
      "24439 -> 30594\n",
      "30594 -> 1426\n",
      "1426 -> 10541\n",
      "10541 -> 1390\n",
      "1390 -> 37336\n",
      "37336 -> 4745\n",
      "4745 -> 1440\n",
      "1440 -> 1535\n",
      "1535 -> 19083\n",
      "19083 -> 29380\n",
      "29380 -> 52597\n",
      "52597 -> 9\n",
      "9 -> 2\n",
      "2 -> 0\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n",
      "0 -> -100\n"
     ]
    }
   ],
   "source": [
    "for l, i in zip(batch[\"labels\"][0].numpy().tolist(), batch[\"input_ids\"][0].numpy().tolist()):\n",
    "    print(f\"{i} -> {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(batch[\"attention_mask\"].numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config.warm_up = 4000\n",
    "training_config.logging_steps = 747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {key: value.to(\"cpu\") for key, value in batch.items()}\n",
    "pred = model(input_ids=batch[\"input_ids\"], attention_mask = batch[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def cal_loss(logits, gold, trg_pad_idx, smoothing=False):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "    n_classes = logits.shape[-1]\n",
    "    logits = logits.view(-1, n_classes)\n",
    "    gold = gold.unsqueeze(-1).view(-1)\n",
    "    loss = F.cross_entropy(logits, gold)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 119])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[\"logits\"].max(-1)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConstrueAutoRegressiveModel(\n",
       "  (model): ConstrueModel(\n",
       "    (token_embeddings): Embedding(60000, 128, padding_idx=0)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-1): 2 x ConstrueDecoderLayer(\n",
       "        (input_norm): LayerNorm()\n",
       "        (self_attn): RopeAttention(\n",
       "          (rope_position_projection): RopePositionEmbedding()\n",
       "          (qkv_projection): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (output_projection): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (post_attention_norm): LayerNorm()\n",
       "        (mlp): PointWiseGatedProjection(\n",
       "          (gate_projection): Linear(in_features=128, out_features=1024, bias=False)\n",
       "          (up_projection): Linear(in_features=128, out_features=1024, bias=False)\n",
       "          (down_projection): Linear(in_features=1024, out_features=128, bias=False)\n",
       "          (act_func): PytorchGELUTanh()\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=128, out_features=60000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_performance(pred, gold, trg_pad_idx=-100):\n",
    "    ''' Apply label smoothing if needed '''\n",
    "\n",
    "    loss = cal_loss(pred, gold, trg_pad_idx)\n",
    "\n",
    "    pred = pred.max(-1)[1].view(-1)\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    non_pad_mask = gold.ne(trg_pad_idx)\n",
    "    n_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\n",
    "    n_word = non_pad_mask.sum().item()\n",
    "\n",
    "    return loss, n_correct, n_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "from typing import Optional, List\n",
    "\n",
    "class WarmupCosineScheduler(LambdaLR):\n",
    "    \"\"\"Linear warmup and cosine decay scheduler.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: AdamW,\n",
    "        warmup_steps: int,\n",
    "        total_steps: int,\n",
    "        min_lr_ratio: float = 0.1,\n",
    "        last_epoch: int = -1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize warmup and decay scheduler.\n",
    "        \n",
    "        Args:\n",
    "            optimizer: AdamW optimizer\n",
    "            warmup_steps: Number of warmup steps\n",
    "            total_steps: Total number of training steps\n",
    "            min_lr_ratio: Minimum learning rate ratio compared to initial lr\n",
    "            last_epoch: The index of the last epoch\n",
    "        \"\"\"\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.min_lr_ratio = min_lr_ratio\n",
    "        super().__init__(optimizer, self.lr_lambda, last_epoch)\n",
    "    \n",
    "    def lr_lambda(self, current_step: int) -> float:\n",
    "        \"\"\"Calculate lr multiplier based on current step.\"\"\"\n",
    "        if current_step < self.warmup_steps:\n",
    "            # Linear warmup\n",
    "            return float(current_step) / float(max(1, self.warmup_steps))\n",
    "        \n",
    "        # Cosine decay\n",
    "        progress = float(current_step - self.warmup_steps) / \\\n",
    "            float(max(1, self.total_steps - self.warmup_steps))\n",
    "        decay = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "        # Scale decay to min_lr_ratio\n",
    "        decay = self.min_lr_ratio + (1.0 - self.min_lr_ratio) * decay\n",
    "        return decay\n",
    "\n",
    "def create_optimizer_and_scheduler(\n",
    "    model: torch.nn.Module,\n",
    "    num_training_steps: int,\n",
    "    learning_rate: float = 5e-5,\n",
    "    weight_decay: float = 0.01,\n",
    "    warmup_ratio: float = 0.1,\n",
    "    min_lr_ratio: float = 0.1,\n",
    "    beta1: float = 0.9,\n",
    "    beta2: float = 0.999,\n",
    "    eps: float = 1e-8,\n",
    "    no_decay_params: Optional[List[str]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create AdamW optimizer and warmup scheduler.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        num_training_steps: Total number of training steps\n",
    "        learning_rate: Maximum learning rate after warmup\n",
    "        weight_decay: Weight decay coefficient\n",
    "        warmup_ratio: Ratio of warmup steps to total steps\n",
    "        min_lr_ratio: Minimum learning rate ratio compared to max lr\n",
    "        beta1: AdamW beta1 parameter\n",
    "        beta2: AdamW beta2 parameter\n",
    "        eps: AdamW epsilon parameter\n",
    "        correct_bias: Whether to correct bias in AdamW\n",
    "        no_decay_params: List of parameter names that should not have weight decay\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (optimizer, scheduler)\n",
    "    \"\"\"\n",
    "    # Default params that should not have weight decay\n",
    "    if no_decay_params is None:\n",
    "        no_decay_params = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "    \n",
    "    # Separate parameters that should and should not have weight decay\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay_params)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay_params)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Create AdamW optimizer\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        betas=(beta1, beta2),\n",
    "        eps=eps\n",
    "    )\n",
    "    \n",
    "    # Create scheduler with linear warmup and cosine decay\n",
    "    warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "    \n",
    "    scheduler = WarmupCosineScheduler(\n",
    "        optimizer=optimizer,\n",
    "        warmup_steps=warmup_steps,\n",
    "        total_steps=num_training_steps,\n",
    "        min_lr_ratio=min_lr_ratio\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler, warmup_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_step(model, batch, optimizer, scheduler, device, logger=None, gradient_accumulation_steps=1):\n",
    "    # forward\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(input_ids=batch[\"input_ids\"].to(device), attention_mask=batch[\"attention_mask\"].to(device))\n",
    "    logits = pred[\"logits\"]\n",
    "    # backward and update parameters\n",
    "    loss, n_correct, n_word = cal_performance(logits, labels) \n",
    "    loss = loss / gradient_accumulation_steps\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    # if (optimizer.step_count + 1) % gradient_accumulation_steps == 0:\n",
    "    #     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    #     optimizer.step()\n",
    "    #     scheduler.step()\n",
    "    #     optimizer.zero_grad()\n",
    "    \n",
    "    return loss.item() * gradient_accumulation_steps, n_correct, n_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_epoch(epoch, model, training_data, optimizer, scheduler, device, logging_steps=-1, logger=None, gradient_accumulation_steps=1):\n",
    "    ''' Epoch operation in training phase'''\n",
    "    model.train()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n",
    "\n",
    "    desc = '  - (Training)   '\n",
    "    for step, batch in tqdm(enumerate(training_data), mininterval=2, desc=desc, leave=False, total=len(train_dataloader)):\n",
    "        # forward\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(input_ids=batch[\"input_ids\"].to(device), attention_mask=batch[\"attention_mask\"].to(device))\n",
    "        logits = pred[\"logits\"]\n",
    "        # backward and update parameters\n",
    "        loss, n_correct, n_word = cal_performance(logits, labels) \n",
    "        \n",
    "        loss.backward()\n",
    "        if (optimizer.step_count + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # note keeping\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        logger.log_metrics(metrics={\n",
    "            'loss': loss.item(),\n",
    "            'epoch': epoch,\n",
    "            \"n_correct\": n_correct,\n",
    "            \"n_word\": n_word,\n",
    "            'learning_rate': sched.get_current_learning_rate()\n",
    "        }, step=step)\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_training_steps(\n",
    "    num_examples: int,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    gradient_accumulation_steps: int = 1\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Calculate the total number of training steps.\n",
    "    \n",
    "    Args:\n",
    "        num_examples: Total number of training examples\n",
    "        num_epochs: Number of epochs to train for\n",
    "        batch_size: Batch size per forward pass\n",
    "        gradient_accumulation_steps: Number of steps to accumulate gradients\n",
    "    \n",
    "    Returns:\n",
    "        int: Total number of optimizer update steps\n",
    "    \"\"\"\n",
    "    steps_per_epoch = math.ceil(num_examples / batch_size)\n",
    "    \n",
    "    update_steps_per_epoch = math.ceil(steps_per_epoch / gradient_accumulation_steps)\n",
    "    \n",
    "    total_training_steps = update_steps_per_epoch * num_epochs\n",
    "    \n",
    "    return total_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class TrainingLogger:\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dir: str,\n",
    "        project_name: str,\n",
    "        log_every_n_steps: int = 100,\n",
    "        save_every_n_steps: int = 1000,\n",
    "        save_best_only: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize training logger with various logging options.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save checkpoints and logs\n",
    "            project_name: Name of the project\n",
    "            use_wandb: Whether to use Weights & Biases logging\n",
    "            log_every_n_steps: How often to log metrics\n",
    "            save_every_n_steps: How often to save checkpoints\n",
    "            save_best_only: Whether to save only the best model\n",
    "        \"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize logging\n",
    "        self.log_file = self.output_dir / 'training.log'\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # Training state\n",
    "        self.global_step = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.start_time = time.time()\n",
    "        self.last_log_time = self.start_time\n",
    "        \n",
    "        # Configuration\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "        self.save_every_n_steps = save_every_n_steps\n",
    "            \n",
    "        # Save configuration\n",
    "        self.save_config({\n",
    "            'output_dir': str(output_dir),\n",
    "            'project_name': project_name,\n",
    "            'log_every_n_steps': log_every_n_steps,\n",
    "            'save_every_n_steps': save_every_n_steps,\n",
    "            'save_best_only': save_best_only,\n",
    "            'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        })\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup logging configuration.\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(self.log_file),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def save_config(self, config: Dict[str, Any]):\n",
    "        \"\"\"Save configuration to JSON file.\"\"\"\n",
    "        config_path = self.output_dir / 'config.json'\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "    \n",
    "    def log_metrics(\n",
    "        self,\n",
    "        metrics: Dict[str, float],\n",
    "        step: Optional[int] = None,\n",
    "        force_log: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Log metrics to all configured outputs.\n",
    "        \n",
    "        Args:\n",
    "            metrics: Dictionary of metric names and values\n",
    "            step: Optional step number (uses global_step if not provided)\n",
    "            force_log: Whether to log regardless of log_every_n_steps\n",
    "        \"\"\"\n",
    "        if step is not None:\n",
    "            self.global_step = step\n",
    "        \n",
    "        # Check if we should log\n",
    "        if not force_log and self.global_step % self.log_every_n_steps != 0:\n",
    "            return\n",
    "        \n",
    "        # Calculate time statistics\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - self.start_time\n",
    "        elapsed_since_last = current_time - self.last_log_time\n",
    "        steps_since_last = self.log_every_n_steps\n",
    "        steps_per_second = steps_since_last / elapsed_since_last\n",
    "        \n",
    "        # Add timing metrics\n",
    "        metrics.update({\n",
    "            'elapsed_time': elapsed,\n",
    "            'steps_per_second': steps_per_second\n",
    "        })\n",
    "        \n",
    "        # Log to terminal and file\n",
    "        log_str = f'Step {self.global_step}: ' + ', '.join(\n",
    "            f'{k}: {v:.4f}' for k, v in metrics.items()\n",
    "        )\n",
    "        logging.info(log_str)\n",
    "        \n",
    "        self.last_log_time = current_time\n",
    "    \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        loss: float,\n",
    "        extra_data: Optional[Dict[str, Any]] = None,\n",
    "        force_save: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save model checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model\n",
    "            optimizer: PyTorch optimizer\n",
    "            loss: Current loss value\n",
    "            extra_data: Additional data to save in checkpoint\n",
    "            force_save: Whether to save regardless of save_every_n_steps\n",
    "        \"\"\"\n",
    "        # Check if we should save\n",
    "        should_save = (\n",
    "            force_save or\n",
    "            self.global_step % self.save_every_n_steps == 0 or\n",
    "            (self.save_best_only and loss < self.best_loss)\n",
    "        )\n",
    "        \n",
    "        if not should_save:\n",
    "            return\n",
    "        \n",
    "        # Update best loss if needed\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "        \n",
    "        # Prepare checkpoint data\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': self.global_step,\n",
    "            'loss': loss,\n",
    "            'best_loss': self.best_loss\n",
    "        }\n",
    "        \n",
    "        if extra_data:\n",
    "            checkpoint.update(extra_data)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = self.output_dir / f'checkpoint_{self.global_step}.pt'\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        logging.info(f'Saved checkpoint at step {self.global_step}')\n",
    "    \n",
    "    def finish(self):\n",
    "        \"\"\"Cleanup and final logging.\"\"\"\n",
    "        total_time = time.time() - self.start_time\n",
    "        logging.info(f'Training finished. Total time: {total_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(experimentation_name,\n",
    "          output_dir,\n",
    "          logging_steps,\n",
    "          save_steps,\n",
    "          num_epochs,\n",
    "          train_dataset,\n",
    "          collate_fn,\n",
    "          batch_size,\n",
    "          gradient_accumulation_steps=1, # not yet implemented\n",
    "          learning_rate=5e-5,\n",
    "          warmup_ratio=0.1,\n",
    "          weight_decay=0.01,\n",
    "          device=\"cuda\"):\n",
    "    \n",
    "    # prepare model\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    train_dataloader = create_data_loader(train_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
    "    \n",
    "    num_training_steps = calculate_training_steps(len(train_dataloader), num_epochs=num_epochs, batch_size=batch_size, gradient_accumulation_steps=gradient_accumulation_steps)\n",
    "\n",
    "    # Create optimizer and scheduler\n",
    "    optimizer, scheduler, warmup_steps = create_optimizer_and_scheduler(\n",
    "        model=model,\n",
    "        num_training_steps=num_training_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    train_logger = TrainingLogger(\n",
    "        project_name=experimentation_name,\n",
    "        output_dir=output_dir,\n",
    "        log_every_n_steps=logging_steps,\n",
    "        save_every_n_steps=save_steps,\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        desc = f'  - (Training)   Epoch {epoch}'\n",
    "        n_word_total, n_word_correct, total_loss = 0, 0, 0\n",
    "        for step, batch in tqdm(enumerate(train_dataloader), mininterval=2, desc=desc, leave=False, total=len(train_dataloader)):\n",
    "            loss, n_correct, n_word = train_step(\n",
    "                model=model,\n",
    "                batch=batch,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "            total_loss += loss\n",
    "            \n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            train_logger.log_metrics(metrics={\n",
    "                'loss': loss,\n",
    "                'epoch': epoch,\n",
    "                \"n_correct\": n_correct,\n",
    "                \"n_word\": n_word,\n",
    "                'learning_rate': current_lr,\n",
    "                \"warmup_steps\": warmup_steps\n",
    "            }, step=step)\n",
    "        print()\n",
    "        print(f\"Epoch {epoch} :: loss per word :: {total_loss/n_word_total}\")\n",
    "    train_logger.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077dbbb07a714b029e1d089a7654adf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  - (Training)   Epoch 0:   0%|          | 0/74787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 08:13:20,512 - INFO - Step 0: loss: 11.1261, epoch: 0.0000, n_correct: 0.0000, n_word: 2181.0000, learning_rate: 0.0000, elapsed_time: 0.3608, steps_per_second: 2771.5616\n",
      "2025-01-19 08:15:19,346 - INFO - Step 1000: loss: 7.5842, epoch: 0.0000, n_correct: 300.0000, n_word: 2404.0000, learning_rate: 0.0000, elapsed_time: 119.1949, steps_per_second: 8.4151\n",
      "2025-01-19 08:17:19,041 - INFO - Step 2000: loss: 6.5283, epoch: 0.0000, n_correct: 385.0000, n_word: 2198.0000, learning_rate: 0.0000, elapsed_time: 238.8901, steps_per_second: 8.3546\n",
      "2025-01-19 08:19:19,875 - INFO - Step 3000: loss: 6.1710, epoch: 0.0000, n_correct: 406.0000, n_word: 2173.0000, learning_rate: 0.0000, elapsed_time: 359.7244, steps_per_second: 8.2758\n",
      "2025-01-19 08:21:17,619 - INFO - Step 4000: loss: 6.1138, epoch: 0.0000, n_correct: 438.0000, n_word: 2346.0000, learning_rate: 0.0000, elapsed_time: 477.4685, steps_per_second: 8.4930\n",
      "2025-01-19 08:23:12,649 - INFO - Step 5000: loss: 5.7809, epoch: 0.0000, n_correct: 497.0000, n_word: 2590.0000, learning_rate: 0.0000, elapsed_time: 592.4976, steps_per_second: 8.6935\n",
      "2025-01-19 08:25:10,540 - INFO - Step 6000: loss: 5.6213, epoch: 0.0000, n_correct: 441.0000, n_word: 1969.0000, learning_rate: 0.0000, elapsed_time: 710.3888, steps_per_second: 8.4824\n",
      "2025-01-19 08:27:08,897 - INFO - Step 7000: loss: 5.7631, epoch: 0.0000, n_correct: 522.0000, n_word: 2570.0000, learning_rate: 0.0000, elapsed_time: 828.7459, steps_per_second: 8.4490\n",
      "2025-01-19 08:29:10,558 - INFO - Step 8000: loss: 5.7749, epoch: 0.0000, n_correct: 501.0000, n_word: 2592.0000, learning_rate: 0.0000, elapsed_time: 950.4069, steps_per_second: 8.2196\n",
      "2025-01-19 08:31:09,879 - INFO - Step 9000: loss: 5.4365, epoch: 0.0000, n_correct: 458.0000, n_word: 2054.0000, learning_rate: 0.0000, elapsed_time: 1069.7279, steps_per_second: 8.3807\n",
      "2025-01-19 08:33:11,153 - INFO - Step 10000: loss: 5.5730, epoch: 0.0000, n_correct: 489.0000, n_word: 2419.0000, learning_rate: 0.0000, elapsed_time: 1191.0021, steps_per_second: 8.2458\n",
      "2025-01-19 08:35:10,677 - INFO - Step 11000: loss: 5.6693, epoch: 0.0000, n_correct: 436.0000, n_word: 1958.0000, learning_rate: 0.0000, elapsed_time: 1310.5263, steps_per_second: 8.3665\n",
      "2025-01-19 08:37:08,904 - INFO - Step 12000: loss: 5.8997, epoch: 0.0000, n_correct: 482.0000, n_word: 2482.0000, learning_rate: 0.0000, elapsed_time: 1428.7536, steps_per_second: 8.4583\n",
      "2025-01-19 08:39:07,226 - INFO - Step 13000: loss: 5.3159, epoch: 0.0000, n_correct: 552.0000, n_word: 2424.0000, learning_rate: 0.0000, elapsed_time: 1547.0753, steps_per_second: 8.4515\n",
      "2025-01-19 08:41:05,106 - INFO - Step 14000: loss: 5.5746, epoch: 0.0000, n_correct: 529.0000, n_word: 2461.0000, learning_rate: 0.0000, elapsed_time: 1664.9554, steps_per_second: 8.4832\n",
      "2025-01-19 08:43:04,549 - INFO - Step 15000: loss: 5.0682, epoch: 0.0000, n_correct: 518.0000, n_word: 2142.0000, learning_rate: 0.0000, elapsed_time: 1784.3984, steps_per_second: 8.3722\n",
      "2025-01-19 08:45:02,475 - INFO - Step 16000: loss: 5.0798, epoch: 0.0000, n_correct: 515.0000, n_word: 1999.0000, learning_rate: 0.0000, elapsed_time: 1902.3244, steps_per_second: 8.4799\n",
      "2025-01-19 08:47:04,547 - INFO - Step 17000: loss: 5.2511, epoch: 0.0000, n_correct: 548.0000, n_word: 2278.0000, learning_rate: 0.0000, elapsed_time: 2024.3961, steps_per_second: 8.1919\n",
      "2025-01-19 08:49:03,932 - INFO - Step 18000: loss: 5.3163, epoch: 0.0000, n_correct: 538.0000, n_word: 2286.0000, learning_rate: 0.0000, elapsed_time: 2143.7816, steps_per_second: 8.3762\n",
      "2025-01-19 08:50:59,916 - INFO - Step 19000: loss: 4.9038, epoch: 0.0000, n_correct: 520.0000, n_word: 2084.0000, learning_rate: 0.0000, elapsed_time: 2259.7652, steps_per_second: 8.6219\n",
      "2025-01-19 08:52:58,381 - INFO - Step 20000: loss: 5.2682, epoch: 0.0000, n_correct: 548.0000, n_word: 2210.0000, learning_rate: 0.0000, elapsed_time: 2378.2304, steps_per_second: 8.4413\n",
      "2025-01-19 08:54:56,018 - INFO - Step 21000: loss: 5.0877, epoch: 0.0000, n_correct: 555.0000, n_word: 2267.0000, learning_rate: 0.0000, elapsed_time: 2495.8667, steps_per_second: 8.5008\n",
      "2025-01-19 08:56:53,590 - INFO - Step 22000: loss: 4.7005, epoch: 0.0000, n_correct: 565.0000, n_word: 2194.0000, learning_rate: 0.0000, elapsed_time: 2613.4394, steps_per_second: 8.5054\n",
      "2025-01-19 08:58:54,527 - INFO - Step 23000: loss: 4.8690, epoch: 0.0000, n_correct: 594.0000, n_word: 2335.0000, learning_rate: 0.0000, elapsed_time: 2734.3763, steps_per_second: 8.2688\n",
      "2025-01-19 09:00:53,560 - INFO - Step 24000: loss: 4.9957, epoch: 0.0000, n_correct: 558.0000, n_word: 2124.0000, learning_rate: 0.0000, elapsed_time: 2853.4095, steps_per_second: 8.4010\n",
      "2025-01-19 09:02:52,331 - INFO - Step 25000: loss: 5.2211, epoch: 0.0000, n_correct: 527.0000, n_word: 2309.0000, learning_rate: 0.0000, elapsed_time: 2972.1803, steps_per_second: 8.4196\n",
      "2025-01-19 09:04:51,743 - INFO - Step 26000: loss: 4.8525, epoch: 0.0000, n_correct: 588.0000, n_word: 2239.0000, learning_rate: 0.0000, elapsed_time: 3091.5918, steps_per_second: 8.3744\n",
      "2025-01-19 09:06:48,430 - INFO - Step 27000: loss: 4.7996, epoch: 0.0000, n_correct: 542.0000, n_word: 1887.0000, learning_rate: 0.0000, elapsed_time: 3208.2786, steps_per_second: 8.5700\n",
      "2025-01-19 09:08:49,877 - INFO - Step 28000: loss: 4.9380, epoch: 0.0000, n_correct: 525.0000, n_word: 1998.0000, learning_rate: 0.0000, elapsed_time: 3329.7257, steps_per_second: 8.2340\n",
      "2025-01-19 09:10:47,592 - INFO - Step 29000: loss: 4.5870, epoch: 0.0000, n_correct: 697.0000, n_word: 2564.0000, learning_rate: 0.0000, elapsed_time: 3447.4413, steps_per_second: 8.4950\n",
      "2025-01-19 09:12:46,997 - INFO - Step 30000: loss: 4.6909, epoch: 0.0000, n_correct: 638.0000, n_word: 2416.0000, learning_rate: 0.0000, elapsed_time: 3566.8460, steps_per_second: 8.3749\n",
      "2025-01-19 09:14:44,832 - INFO - Step 31000: loss: 4.6856, epoch: 0.0000, n_correct: 598.0000, n_word: 2117.0000, learning_rate: 0.0000, elapsed_time: 3684.6813, steps_per_second: 8.4864\n",
      "2025-01-19 09:16:43,832 - INFO - Step 32000: loss: 4.7878, epoch: 0.0000, n_correct: 614.0000, n_word: 2441.0000, learning_rate: 0.0000, elapsed_time: 3803.6815, steps_per_second: 8.4033\n",
      "2025-01-19 09:18:42,339 - INFO - Step 33000: loss: 4.7256, epoch: 0.0000, n_correct: 550.0000, n_word: 2152.0000, learning_rate: 0.0000, elapsed_time: 3922.1878, steps_per_second: 8.4384\n",
      "2025-01-19 09:20:41,716 - INFO - Step 34000: loss: 4.5742, epoch: 0.0000, n_correct: 630.0000, n_word: 2228.0000, learning_rate: 0.0000, elapsed_time: 4041.5647, steps_per_second: 8.3768\n",
      "2025-01-19 09:22:42,838 - INFO - Step 35000: loss: 4.6395, epoch: 0.0000, n_correct: 601.0000, n_word: 2171.0000, learning_rate: 0.0000, elapsed_time: 4162.6873, steps_per_second: 8.2561\n",
      "2025-01-19 09:24:41,681 - INFO - Step 36000: loss: 4.7001, epoch: 0.0000, n_correct: 556.0000, n_word: 1870.0000, learning_rate: 0.0000, elapsed_time: 4281.5304, steps_per_second: 8.4145\n",
      "2025-01-19 09:26:36,792 - INFO - Step 37000: loss: 4.6198, epoch: 0.0000, n_correct: 580.0000, n_word: 2114.0000, learning_rate: 0.0000, elapsed_time: 4396.6407, steps_per_second: 8.6873\n",
      "2025-01-19 09:28:36,464 - INFO - Step 38000: loss: 4.4665, epoch: 0.0000, n_correct: 713.0000, n_word: 2488.0000, learning_rate: 0.0000, elapsed_time: 4516.3134, steps_per_second: 8.3561\n",
      "2025-01-19 09:30:34,744 - INFO - Step 39000: loss: 4.8569, epoch: 0.0000, n_correct: 513.0000, n_word: 2119.0000, learning_rate: 0.0000, elapsed_time: 4634.5934, steps_per_second: 8.4545\n",
      "2025-01-19 09:32:34,614 - INFO - Step 40000: loss: 4.8627, epoch: 0.0000, n_correct: 623.0000, n_word: 2462.0000, learning_rate: 0.0000, elapsed_time: 4754.4630, steps_per_second: 8.3424\n",
      "2025-01-19 09:34:34,280 - INFO - Step 41000: loss: 4.5318, epoch: 0.0000, n_correct: 696.0000, n_word: 2319.0000, learning_rate: 0.0000, elapsed_time: 4874.1288, steps_per_second: 8.3566\n",
      "2025-01-19 09:36:32,602 - INFO - Step 42000: loss: 4.4519, epoch: 0.0000, n_correct: 767.0000, n_word: 2524.0000, learning_rate: 0.0000, elapsed_time: 4992.4507, steps_per_second: 8.4515\n",
      "2025-01-19 09:38:30,482 - INFO - Step 43000: loss: 4.5382, epoch: 0.0000, n_correct: 676.0000, n_word: 2332.0000, learning_rate: 0.0000, elapsed_time: 5110.3313, steps_per_second: 8.4832\n",
      "2025-01-19 09:40:26,110 - INFO - Step 44000: loss: 4.7279, epoch: 0.0000, n_correct: 611.0000, n_word: 2198.0000, learning_rate: 0.0000, elapsed_time: 5225.9591, steps_per_second: 8.6484\n",
      "2025-01-19 09:42:24,551 - INFO - Step 45000: loss: 4.3221, epoch: 0.0000, n_correct: 591.0000, n_word: 1965.0000, learning_rate: 0.0000, elapsed_time: 5344.4002, steps_per_second: 8.4430\n",
      "2025-01-19 09:44:22,009 - INFO - Step 46000: loss: 4.5642, epoch: 0.0000, n_correct: 607.0000, n_word: 2194.0000, learning_rate: 0.0000, elapsed_time: 5461.8584, steps_per_second: 8.5137\n",
      "2025-01-19 09:46:21,529 - INFO - Step 47000: loss: 4.6846, epoch: 0.0000, n_correct: 606.0000, n_word: 2175.0000, learning_rate: 0.0000, elapsed_time: 5581.3776, steps_per_second: 8.3669\n",
      "2025-01-19 09:48:20,629 - INFO - Step 48000: loss: 4.4548, epoch: 0.0000, n_correct: 915.0000, n_word: 2980.0000, learning_rate: 0.0000, elapsed_time: 5700.4785, steps_per_second: 8.3962\n",
      "2025-01-19 09:50:18,551 - INFO - Step 49000: loss: 4.5768, epoch: 0.0000, n_correct: 628.0000, n_word: 2341.0000, learning_rate: 0.0000, elapsed_time: 5818.4001, steps_per_second: 8.4802\n",
      "2025-01-19 09:52:15,601 - INFO - Step 50000: loss: 4.4300, epoch: 0.0000, n_correct: 633.0000, n_word: 2220.0000, learning_rate: 0.0000, elapsed_time: 5935.4499, steps_per_second: 8.5434\n",
      "2025-01-19 09:54:14,602 - INFO - Step 51000: loss: 4.9783, epoch: 0.0000, n_correct: 556.0000, n_word: 2218.0000, learning_rate: 0.0000, elapsed_time: 6054.4516, steps_per_second: 8.4032\n",
      "2025-01-19 09:56:12,764 - INFO - Step 52000: loss: 4.7508, epoch: 0.0000, n_correct: 582.0000, n_word: 2112.0000, learning_rate: 0.0000, elapsed_time: 6172.6128, steps_per_second: 8.4630\n",
      "2025-01-19 09:58:10,789 - INFO - Step 53000: loss: 4.1543, epoch: 0.0000, n_correct: 643.0000, n_word: 1894.0000, learning_rate: 0.0000, elapsed_time: 6290.6382, steps_per_second: 8.4728\n",
      "2025-01-19 10:00:09,106 - INFO - Step 54000: loss: 4.4759, epoch: 0.0000, n_correct: 553.0000, n_word: 1956.0000, learning_rate: 0.0000, elapsed_time: 6408.9547, steps_per_second: 8.4519\n",
      "2025-01-19 10:02:06,961 - INFO - Step 55000: loss: 4.6134, epoch: 0.0000, n_correct: 640.0000, n_word: 2215.0000, learning_rate: 0.0000, elapsed_time: 6526.8097, steps_per_second: 8.4850\n",
      "2025-01-19 10:04:04,121 - INFO - Step 56000: loss: 4.3529, epoch: 0.0000, n_correct: 589.0000, n_word: 2056.0000, learning_rate: 0.0000, elapsed_time: 6643.9698, steps_per_second: 8.5353\n",
      "2025-01-19 10:06:01,464 - INFO - Step 57000: loss: 4.2794, epoch: 0.0000, n_correct: 684.0000, n_word: 2338.0000, learning_rate: 0.0000, elapsed_time: 6761.3133, steps_per_second: 8.5220\n",
      "2025-01-19 10:08:01,105 - INFO - Step 58000: loss: 4.0324, epoch: 0.0000, n_correct: 823.0000, n_word: 2502.0000, learning_rate: 0.0000, elapsed_time: 6880.9546, steps_per_second: 8.3583\n",
      "2025-01-19 10:10:01,963 - INFO - Step 59000: loss: 4.3218, epoch: 0.0000, n_correct: 713.0000, n_word: 2329.0000, learning_rate: 0.0000, elapsed_time: 7001.8124, steps_per_second: 8.2742\n",
      "2025-01-19 10:12:00,612 - INFO - Step 60000: loss: 4.4752, epoch: 0.0000, n_correct: 680.0000, n_word: 2355.0000, learning_rate: 0.0000, elapsed_time: 7120.4607, steps_per_second: 8.4283\n",
      "2025-01-19 10:13:59,503 - INFO - Step 61000: loss: 4.6459, epoch: 0.0000, n_correct: 679.0000, n_word: 2431.0000, learning_rate: 0.0000, elapsed_time: 7239.3523, steps_per_second: 8.4110\n",
      "2025-01-19 10:15:56,894 - INFO - Step 62000: loss: 4.4396, epoch: 0.0000, n_correct: 614.0000, n_word: 2092.0000, learning_rate: 0.0000, elapsed_time: 7356.7429, steps_per_second: 8.5186\n",
      "2025-01-19 10:17:54,905 - INFO - Step 63000: loss: 4.5069, epoch: 0.0000, n_correct: 660.0000, n_word: 2269.0000, learning_rate: 0.0000, elapsed_time: 7474.7537, steps_per_second: 8.4738\n",
      "2025-01-19 10:19:54,803 - INFO - Step 64000: loss: 4.5851, epoch: 0.0000, n_correct: 663.0000, n_word: 2401.0000, learning_rate: 0.0000, elapsed_time: 7594.6518, steps_per_second: 8.3404\n",
      "2025-01-19 10:21:55,094 - INFO - Step 65000: loss: 4.4775, epoch: 0.0000, n_correct: 669.0000, n_word: 2271.0000, learning_rate: 0.0000, elapsed_time: 7714.9427, steps_per_second: 8.3132\n",
      "2025-01-19 10:23:55,967 - INFO - Step 66000: loss: 4.4011, epoch: 0.0000, n_correct: 649.0000, n_word: 2097.0000, learning_rate: 0.0000, elapsed_time: 7835.8158, steps_per_second: 8.2731\n",
      "2025-01-19 10:25:54,174 - INFO - Step 67000: loss: 4.2620, epoch: 0.0000, n_correct: 848.0000, n_word: 2782.0000, learning_rate: 0.0000, elapsed_time: 7954.0233, steps_per_second: 8.4597\n",
      "2025-01-19 10:27:53,147 - INFO - Step 68000: loss: 4.3724, epoch: 0.0000, n_correct: 773.0000, n_word: 2591.0000, learning_rate: 0.0000, elapsed_time: 8072.9958, steps_per_second: 8.4053\n",
      "2025-01-19 10:29:52,588 - INFO - Step 69000: loss: 4.4732, epoch: 0.0000, n_correct: 672.0000, n_word: 2331.0000, learning_rate: 0.0000, elapsed_time: 8192.4374, steps_per_second: 8.3723\n",
      "2025-01-19 10:31:51,881 - INFO - Step 70000: loss: 4.3063, epoch: 0.0000, n_correct: 688.0000, n_word: 2196.0000, learning_rate: 0.0000, elapsed_time: 8311.7305, steps_per_second: 8.3827\n",
      "2025-01-19 10:33:49,709 - INFO - Step 71000: loss: 4.4756, epoch: 0.0000, n_correct: 610.0000, n_word: 2066.0000, learning_rate: 0.0000, elapsed_time: 8429.5582, steps_per_second: 8.4870\n",
      "2025-01-19 10:35:49,076 - INFO - Step 72000: loss: 4.3445, epoch: 0.0000, n_correct: 630.0000, n_word: 2054.0000, learning_rate: 0.0000, elapsed_time: 8548.9250, steps_per_second: 8.3775\n",
      "2025-01-19 10:37:48,103 - INFO - Step 73000: loss: 4.3034, epoch: 0.0000, n_correct: 616.0000, n_word: 2052.0000, learning_rate: 0.0000, elapsed_time: 8667.9519, steps_per_second: 8.4015\n",
      "2025-01-19 10:39:47,754 - INFO - Step 74000: loss: 4.2784, epoch: 0.0000, n_correct: 722.0000, n_word: 2473.0000, learning_rate: 0.0000, elapsed_time: 8787.6030, steps_per_second: 8.3576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 :: loss per word :: 0.0021391541398447806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa00de8692845068b2448994fa6b7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  - (Training)   Epoch 1:   0%|          | 0/74787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 10:41:40,785 - INFO - Step 0: loss: 4.4777, epoch: 1.0000, n_correct: 653.0000, n_word: 2231.0000, learning_rate: 0.0000, elapsed_time: 8900.6342, steps_per_second: 8.8471\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x740b75b56fc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/ai_experimentation_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/anaconda3/envs/ai_experimentation_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/envs/ai_experimentation_env/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x740b75b56fc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/ai_experimentation_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/anaconda3/envs/ai_experimentation_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/envs/ai_experimentation_env/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x740b75b56fc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/ai_experimentation_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/root/anaconda3/envs/ai_experimentation_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/envs/ai_experimentation_env/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "2025-01-19 10:43:37,693 - INFO - Step 1000: loss: 4.4594, epoch: 1.0000, n_correct: 612.0000, n_word: 2141.0000, learning_rate: 0.0000, elapsed_time: 9017.5421, steps_per_second: 8.5537\n",
      "2025-01-19 10:45:33,910 - INFO - Step 2000: loss: 4.2591, epoch: 1.0000, n_correct: 660.0000, n_word: 2005.0000, learning_rate: 0.0000, elapsed_time: 9133.7593, steps_per_second: 8.6046\n",
      "2025-01-19 10:47:33,659 - INFO - Step 3000: loss: 4.0903, epoch: 1.0000, n_correct: 694.0000, n_word: 2166.0000, learning_rate: 0.0000, elapsed_time: 9253.5080, steps_per_second: 8.3508\n",
      "2025-01-19 10:49:32,509 - INFO - Step 4000: loss: 4.2696, epoch: 1.0000, n_correct: 715.0000, n_word: 2295.0000, learning_rate: 0.0000, elapsed_time: 9372.3584, steps_per_second: 8.4139\n",
      "2025-01-19 10:51:31,800 - INFO - Step 5000: loss: 4.3528, epoch: 1.0000, n_correct: 705.0000, n_word: 2270.0000, learning_rate: 0.0000, elapsed_time: 9491.6491, steps_per_second: 8.3829\n",
      "2025-01-19 10:53:30,982 - INFO - Step 6000: loss: 4.4004, epoch: 1.0000, n_correct: 620.0000, n_word: 2045.0000, learning_rate: 0.0000, elapsed_time: 9610.8312, steps_per_second: 8.3905\n",
      "2025-01-19 10:55:29,210 - INFO - Step 7000: loss: 4.2450, epoch: 1.0000, n_correct: 903.0000, n_word: 2920.0000, learning_rate: 0.0000, elapsed_time: 9729.0593, steps_per_second: 8.4582\n",
      "2025-01-19 10:57:25,573 - INFO - Step 8000: loss: 4.3420, epoch: 1.0000, n_correct: 643.0000, n_word: 2145.0000, learning_rate: 0.0000, elapsed_time: 9845.4219, steps_per_second: 8.5938\n",
      "2025-01-19 10:59:23,058 - INFO - Step 9000: loss: 4.1204, epoch: 1.0000, n_correct: 737.0000, n_word: 2301.0000, learning_rate: 0.0000, elapsed_time: 9962.9069, steps_per_second: 8.5117\n",
      "2025-01-19 11:01:21,236 - INFO - Step 10000: loss: 4.4733, epoch: 1.0000, n_correct: 652.0000, n_word: 2299.0000, learning_rate: 0.0000, elapsed_time: 10081.0850, steps_per_second: 8.4618\n",
      "2025-01-19 11:03:20,285 - INFO - Step 11000: loss: 4.2279, epoch: 1.0000, n_correct: 645.0000, n_word: 2040.0000, learning_rate: 0.0000, elapsed_time: 10200.1336, steps_per_second: 8.3999\n",
      "2025-01-19 11:05:18,243 - INFO - Step 12000: loss: 4.3885, epoch: 1.0000, n_correct: 700.0000, n_word: 2374.0000, learning_rate: 0.0000, elapsed_time: 10318.0919, steps_per_second: 8.4776\n",
      "2025-01-19 11:07:14,496 - INFO - Step 13000: loss: 3.9267, epoch: 1.0000, n_correct: 770.0000, n_word: 2194.0000, learning_rate: 0.0000, elapsed_time: 10434.3450, steps_per_second: 8.6019\n",
      "2025-01-19 11:09:13,357 - INFO - Step 14000: loss: 4.2879, epoch: 1.0000, n_correct: 642.0000, n_word: 2222.0000, learning_rate: 0.0000, elapsed_time: 10553.2058, steps_per_second: 8.4132\n",
      "2025-01-19 11:11:12,007 - INFO - Step 15000: loss: 4.2024, epoch: 1.0000, n_correct: 656.0000, n_word: 2048.0000, learning_rate: 0.0000, elapsed_time: 10671.8559, steps_per_second: 8.4281\n",
      "2025-01-19 11:13:12,128 - INFO - Step 16000: loss: 4.2026, epoch: 1.0000, n_correct: 559.0000, n_word: 1782.0000, learning_rate: 0.0000, elapsed_time: 10791.9773, steps_per_second: 8.3249\n",
      "2025-01-19 11:15:11,018 - INFO - Step 17000: loss: 4.3297, epoch: 1.0000, n_correct: 797.0000, n_word: 2634.0000, learning_rate: 0.0000, elapsed_time: 10910.8667, steps_per_second: 8.4112\n",
      "2025-01-19 11:17:11,885 - INFO - Step 18000: loss: 4.5067, epoch: 1.0000, n_correct: 674.0000, n_word: 2251.0000, learning_rate: 0.0000, elapsed_time: 11031.7344, steps_per_second: 8.2735\n",
      "2025-01-19 11:19:10,198 - INFO - Step 19000: loss: 4.3180, epoch: 1.0000, n_correct: 609.0000, n_word: 1914.0000, learning_rate: 0.0000, elapsed_time: 11150.0475, steps_per_second: 8.4521\n",
      "2025-01-19 11:21:11,437 - INFO - Step 20000: loss: 4.2169, epoch: 1.0000, n_correct: 880.0000, n_word: 2835.0000, learning_rate: 0.0000, elapsed_time: 11271.2859, steps_per_second: 8.2482\n",
      "2025-01-19 11:23:10,262 - INFO - Step 21000: loss: 4.0214, epoch: 1.0000, n_correct: 680.0000, n_word: 1998.0000, learning_rate: 0.0000, elapsed_time: 11390.1114, steps_per_second: 8.4157\n",
      "2025-01-19 11:25:08,655 - INFO - Step 22000: loss: 4.5519, epoch: 1.0000, n_correct: 685.0000, n_word: 2388.0000, learning_rate: 0.0000, elapsed_time: 11508.5038, steps_per_second: 8.4465\n",
      "2025-01-19 11:27:04,967 - INFO - Step 23000: loss: 4.1960, epoch: 1.0000, n_correct: 702.0000, n_word: 2253.0000, learning_rate: 0.0000, elapsed_time: 11624.8164, steps_per_second: 8.5975\n",
      "2025-01-19 11:29:03,302 - INFO - Step 24000: loss: 4.3910, epoch: 1.0000, n_correct: 764.0000, n_word: 2493.0000, learning_rate: 0.0000, elapsed_time: 11743.1513, steps_per_second: 8.4506\n",
      "2025-01-19 11:30:59,734 - INFO - Step 25000: loss: 4.2227, epoch: 1.0000, n_correct: 820.0000, n_word: 2701.0000, learning_rate: 0.0000, elapsed_time: 11859.5828, steps_per_second: 8.5887\n",
      "2025-01-19 11:32:58,424 - INFO - Step 26000: loss: 4.2673, epoch: 1.0000, n_correct: 781.0000, n_word: 2574.0000, learning_rate: 0.0000, elapsed_time: 11978.2726, steps_per_second: 8.4253\n",
      "2025-01-19 11:34:58,605 - INFO - Step 27000: loss: 4.3264, epoch: 1.0000, n_correct: 790.0000, n_word: 2524.0000, learning_rate: 0.0000, elapsed_time: 12098.4542, steps_per_second: 8.3207\n",
      "2025-01-19 11:36:58,964 - INFO - Step 28000: loss: 4.1725, epoch: 1.0000, n_correct: 726.0000, n_word: 2258.0000, learning_rate: 0.0000, elapsed_time: 12218.8131, steps_per_second: 8.3085\n",
      "2025-01-19 11:38:56,668 - INFO - Step 29000: loss: 4.2002, epoch: 1.0000, n_correct: 657.0000, n_word: 2109.0000, learning_rate: 0.0000, elapsed_time: 12336.5175, steps_per_second: 8.4959\n",
      "2025-01-19 11:40:56,659 - INFO - Step 30000: loss: 4.0469, epoch: 1.0000, n_correct: 746.0000, n_word: 2317.0000, learning_rate: 0.0000, elapsed_time: 12456.5082, steps_per_second: 8.3340\n"
     ]
    }
   ],
   "source": [
    "dataset_config.batch_size\n",
    "\n",
    "\n",
    "collate_fn = NextTokenPredictionCollator(tokenizer=tokenizer)\n",
    "\n",
    "training_config.logging_steps = 1000\n",
    "training_config.warm_up = 4000\n",
    "\n",
    "train(\n",
    "    experimentation_name=training_config.experimentation_name,\n",
    "    output_dir=training_config.save_path,\n",
    "    logging_steps=training_config.logging_steps,\n",
    "    save_steps=training_config.logging_steps,\n",
    "    num_epochs=training_config.num_epochs,\n",
    "    train_dataset=train_examples_pt,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=dataset_config.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"weights.pt\", \"wb\") as handler:\n",
    "    torch.save(model.state_dict(), handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
